'''*
tags::math

input:
import numpy as np
def RMS_prop_step(grads, layers):
    """
    Root mean square propagation step.
    """
    alpha = 0.5
    beta = 1.0
    s = [np.zeros_like(grad) for grad in grads]
    for i, (grad, layer) in enumerate(zip(grads, layers)):
        s[i] = beta*s[i] + (1-beta)*grad**2
        grad = alpha * (grad/(np.sqrt(s[i])))
        layer = layer+grad
    return layers

output:
def RMS_prop_step(grads, layers):
    """
    Root mean square propagation step.
    """
    from Compiler import mpc_math

    alpha = sfix(0.5)
    beta = sfix(1.0)
    s = [sfix.Tensor(grad.shape) for grad in grads]
    for i, (grad, layer) in enumerate(zip(grads, layers)):
        s_vec = s[i].get_vector()
        grad_vec = grad.get_vector()
        layer_vec = layer.get_vector()
        s_vec = beta*s_vec + (1-beta)*grad_vec**2
        grad_vec = alpha * (grad_vec/mpc_math.sqrt(s_vec))
        layer_vec = layer_vec+grad_vec
        layer.assign_vector(layer_vec)
    return layers

*'''

####################################################################

# insert here #
from Compiler.mpc_math import sqrt

def RMS_prop_step(grads, layers):
    alpha = sfix(0.5)
    beta = sfix(1.0)
    s = [sfix.Tensor(grad.shape) for grad in grads]
    for grad in s:
        grad.assign_all(0)
    for i, (grad, layer) in enumerate(zip(grads, layers)):
        s[i].assign_vector(beta * s[i].get_vector() + (1 - beta) * grad.get_vector() ** 2)
        grad_vec = grad.get_vector()
        grad_vec = alpha.get_vector() * (grad_vec / sqrt(s[i].get_vector()))
        grad.assign_vector(grad_vec)
        layer.assign_vector(layer.get_vector() + grad_vec)
    return layers

####################################################################

import numpy as np
def standard(grads, layers):
    """
    Root mean square propagation step.
    """
    alpha = 0.5
    beta = 1.0
    s = [np.zeros_like(grad) for grad in grads]
    for i, (grad, layer) in enumerate(zip(grads, layers)):
        s[i] = beta*s[i] + (1-beta)*grad**2
        grad = alpha * (grad/(np.sqrt(s[i])))
        layer = layer+grad
    return layers

layer1 = np.array([[1.0,2.0],[2.0,1.0]])
layer2 = np.array([[1.0,2.0,3.0],[2.0,1.0,0.0]])
grad1 = np.array([[0.5,0.5],[-0.5,0.5]])
grad2 = np.array([[0.2,0.1,-0.1],[0.15,0.2,0.2]])
layers = [layer1,layer2]
grads = [grad1,grad2]

slayer1 = sfix.Tensor(layer1.shape)
slayer2 = sfix.Tensor(layer2.shape)
sgrad1 = sfix.Tensor(grad1.shape)
sgrad2 = sfix.Tensor(grad2.shape)

for i in range(layer1.shape[0]):
    for j in range(layer1.shape[1]):
        slayer1[i][j] = layer1[i][j]
        sgrad1[i][j] = grad1[i][j]
for i in range(layer2.shape[0]):
    for j in range(layer2.shape[1]):
        slayer2[i][j] = layer2[i][j]
        sgrad2[i][j] = grad2[i][j]

slayers = [slayer1,slayer2]
sgrads = [sgrad1,sgrad2]

expected_layers = standard(grads,layers)
actual_layers = RMS_prop_step(sgrads,slayers)

flag = sint(1)
for k in range(2):
    for i in range(actual_layers[k].shape[0]):
        for j in range(actual_layers[k].shape[1]):
            flag = flag.bit_and(abs(actual_layers[k][i][j]-expected_layers[k][i][j])<0.05)

@if_e(flag.reveal())
def _():
    print_ln("Pass the test")
@else_
def _():
    pass
